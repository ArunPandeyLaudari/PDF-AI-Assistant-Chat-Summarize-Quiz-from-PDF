{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928e7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35996fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "LLAMA_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eca10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while parsing the file 'sample.pdf': [Errno 2] No such file or directory: 'd:/ML(ExtraClass Project)/RAG_PROJECT/PDF-AI-Assistant-Chat-Summarize-Quiz-from-PDF/notebooks/sample.pdf'\n",
      "✅ Saved 0 markdown files to 'markdown'\n"
     ]
    }
   ],
   "source": [
    "parsing_instruction = \"\"\"\n",
    "You are a document parser. Extract the content into clean, structured markdown format.\n",
    "- Preserve headings, subheadings, paragraphs clearly.\n",
    "- Convert tables into proper markdown table syntax.\n",
    "- Represent images with markdown image syntax ![Description](image_placeholder).\n",
    "- If image data is missing, describe the image briefly in place.\n",
    "- Keep lists, bullet points, and code blocks formatted.\n",
    "- Avoid extra line breaks or broken markdown syntax.\n",
    "\"\"\"\n",
    "\n",
    "def parse_single_pdf(pdf_path: str) -> list:\n",
    "    parser = LlamaParse(\n",
    "        api_key=LLAMA_API_KEY,\n",
    "        result_type=\"markdown\",\n",
    "        verbose=True,\n",
    "        parsing_instruction=parsing_instruction,\n",
    "    )\n",
    "    return parser.load_data(pdf_path)\n",
    "\n",
    "def save_markdown(docs, folder=\"markdown\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, doc in enumerate(docs):\n",
    "        with open(os.path.join(folder, f\"doc_{i+1}.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc.text)\n",
    "    print(f\"✅ Saved {len(docs)} markdown files to '{folder}'\")\n",
    "\n",
    "pdf_path = \"sample.pdf\"  # ✅ Provide full path to single PDF\n",
    "docs = parse_single_pdf(pdf_path)\n",
    "save_markdown(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644c8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document as LCDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4419ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 12 markdown documents\n"
     ]
    }
   ],
   "source": [
    "# Load markdown as LangChain documents\n",
    "def load_markdown_docs(folder=\"markdown\"):\n",
    "    loaded_docs = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".md\"):\n",
    "            with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            loaded_docs.append(LCDocument(page_content=text, metadata={\"source\": filename}))\n",
    "    return loaded_docs\n",
    "\n",
    "markdown_docs = load_markdown_docs()\n",
    "print(f\"✅ Loaded {len(markdown_docs)} markdown documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bf1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2601f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "\n",
    "chunked_docs = []\n",
    "for doc in markdown_docs:\n",
    "    for i, chunk in enumerate(child_splitter.split_text(doc.page_content)):\n",
    "        chunked_docs.append(LCDocument(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": doc.metadata[\"source\"], \"chunk\": i}\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b4cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML(ExtraClass Project)\\RAG_PROJECT\\PDF-AI-Assistant-Chat-Summarize-Quiz-from-PDF\\.pdfassistantenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\arunp\\AppData\\Local\\Temp\\ipykernel_9428\\663433846.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 71 chunks in Chroma vector store.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"md_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(chunked_docs)\n",
    "print(f\"✅ Stored {len(chunked_docs)} chunks in Chroma vector store.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f11369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce60fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc424741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ParentDocumentRetriever ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "retriever.add_documents(markdown_docs)\n",
    "print(\"✅ ParentDocumentRetriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67bcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f5ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Use the context below to answer the question.\n",
    "If the answer is not found, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert summarizer. Read the context and summarize key points, headings, and lists.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "quiz_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a quiz generator. Create {num_questions} MCQs from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac74fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03bd343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206a047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a8665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116ae9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatters\n",
    "context_retriever = RunnableLambda(lambda x: {\"context\": retriever.get_relevant_documents(x.get(\"question\", \"\")), **x})\n",
    "\n",
    "chat_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", [])),\n",
    "    \"question\": x.get(\"question\", \"\")\n",
    "})\n",
    "\n",
    "summarize_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", []))\n",
    "})\n",
    "\n",
    "quiz_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", [])),\n",
    "    \"num_questions\": x.get(\"num_questions\", 5)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f08c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "chat_chain = RunnableSequence(context_retriever, chat_formatter, chat_prompt, llm, parser)\n",
    "summarize_chain = RunnableSequence(context_retriever, summarize_formatter, summarize_prompt, llm, parser)\n",
    "quiz_chain = RunnableSequence(context_retriever, quiz_formatter, quiz_prompt, llm, parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ac9fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chat(x): return x.get(\"mode\") == \"chat\"\n",
    "def is_summary(x): return x.get(\"mode\") == \"summarize\"\n",
    "def is_quiz(x): return x.get(\"mode\") == \"quiz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a513c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RunnableBranch(\n",
    "    (is_chat, chat_chain),\n",
    "    (is_summary, summarize_chain),\n",
    "    (is_quiz, quiz_chain),\n",
    "    RunnableLambda(lambda _: \"❌ Invalid mode. Choose 'chat', 'summarize', or 'quiz'.\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d75551fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunp\\AppData\\Local\\Temp\\ipykernel_9428\\3374979016.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_retriever = RunnableLambda(lambda x: {\"context\": retriever.get_relevant_documents(x.get(\"question\", \"\")), **x})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗣️ Chat: A microcontroller is a highly integrated device which includes, on one chip, all or most of the parts needed to perform an application control function. It typically has bit manipulation instructions, easy and direct access to I/O, and quick and efficient interrupt processing.\n",
      "\n",
      "🧾 Summary: **Summary of Key Points:**\n",
      "\n",
      "1. **Microcontrollers Made Easy**: The article covers the basics of microcontrollers, including memory types and CPU functionality.\n",
      "2. **Memory Types**:\n",
      "\t* Flash: Electrically erasable and programmable memory, with endurance of 100 or 300,000 program/erase cycles.\n",
      "\t* RAM (Random Access Memory): Stores data temporarily during program execution.\n",
      "\t* EEPROM (Electrically Erasable Programmable Read Only Memory): Stores data that must be saved through a power down cycle.\n",
      "3. **CPU (Central Processing Unit)**: The brain of the system that processes data and executes instructions.\n",
      "4. **Communication**:\n",
      "\t* CAN (Controller Area Network): A multiplexed wiring scheme developed for automobiles, with potential for industrial control applications.\n",
      "\t* J1850: The SAE multiplexed automotive wiring standard used in North America.\n",
      "5. **Typical Microcontroller Block Organization**: A diagram showing the various components of a microcontroller.\n",
      "\n",
      "**Key Headings:**\n",
      "\n",
      "1. **Flash**\n",
      "2. **RAM (Random Access Memory)**\n",
      "3. **EEPROM (Electrically Erasable Programmable Read Only Memory)**\n",
      "4. **CPU (Central Processing Unit)**\n",
      "5. **2.4 COMMUNICATION**\n",
      "6. **Typical Microcontroller Block Organization**\n",
      "\n",
      "**Key Lists:**\n",
      "\n",
      "1. **CAN & J1850**:\n",
      "\t* CAN: A multiplexed wiring scheme developed for automobiles.\n",
      "\t* J1850: The SAE multiplexed automotive wiring standard used in North America.\n",
      "2. **Inter System | Fast Speed | Slow Speed**:\n",
      "\t* A table showing the different systems and their corresponding speeds in a vehicle.\n",
      "\n",
      "🧠 Quiz:\n",
      " Here are 3 MCQs based on the given context:\n",
      "\n",
      "**MCQ 1**\n",
      "What is polling in the context of microcontrollers?\n",
      "\n",
      "A) A hardware technique to interrupt the controller when data is ready\n",
      "B) A software technique to continually ask peripherals if they need servicing\n",
      "C) A method to save the state of the controller when an event occurs\n",
      "D) A way to restore the state of the controller after an interrupt\n",
      "\n",
      "**Answer: B) A software technique to continually ask peripherals if they need servicing**\n",
      "\n",
      "**MCQ 2**\n",
      "What is the main advantage of using interrupts over polling in microcontrollers?\n",
      "\n",
      "A) Interrupts are more complex to implement\n",
      "B) Interrupts consume more power than polling\n",
      "C) Interrupts are more efficient and reduce CPU usage\n",
      "D) Interrupts are only used in time-critical applications\n",
      "\n",
      "**Answer: C) Interrupts are more efficient and reduce CPU usage**\n",
      "\n",
      "**MCQ 3**\n",
      "What is the primary purpose of microcontrollers?\n",
      "\n",
      "A) To achieve maximum processing performance\n",
      "B) To implement a set of control functions in the most cost-effective way\n",
      "C) To handle multiple tasks simultaneously\n",
      "D) To provide a high level of security\n",
      "\n",
      "**Answer: B) To implement a set of control functions in the most cost-effective way**\n"
     ]
    }
   ],
   "source": [
    "chat_response = rag_chain.invoke({\n",
    "    \"mode\": \"chat\",\n",
    "    \"question\": \"What is a microcontroller?\"\n",
    "})\n",
    "print(\"🗣️ Chat:\", chat_response)\n",
    "\n",
    "summary_response = rag_chain.invoke({\n",
    "    \"mode\": \"summarize\",\n",
    "    \"question\": \"Summarize this\"  # for retrieval\n",
    "})\n",
    "print(\"\\n🧾 Summary:\", summary_response)\n",
    "\n",
    "quiz_response = rag_chain.invoke({\n",
    "    \"mode\": \"quiz\",\n",
    "    \"question\": \"Generate quiz\",\n",
    "    \"num_questions\": 3\n",
    "})\n",
    "print(\"\\n🧠 Quiz:\\n\", quiz_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146863d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pdfassistantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
