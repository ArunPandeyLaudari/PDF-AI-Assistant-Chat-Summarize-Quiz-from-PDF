{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928e7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35996fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "LLAMA_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eca10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while parsing the file 'sample.pdf': [Errno 2] No such file or directory: 'd:/ML(ExtraClass Project)/RAG_PROJECT/PDF-AI-Assistant-Chat-Summarize-Quiz-from-PDF/notebooks/sample.pdf'\n",
      " Saved 0 markdown files to 'markdown'\n"
     ]
    }
   ],
   "source": [
    "parsing_instruction = \"\"\"\n",
    "You are a document parser. Extract the content into clean, structured markdown format.\n",
    "- Preserve headings, subheadings, paragraphs clearly.\n",
    "- Convert tables into proper markdown table syntax.\n",
    "- Represent images with markdown image syntax ![Description](image_placeholder).\n",
    "- If image data is missing, describe the image briefly in place.\n",
    "- Keep lists, bullet points, and code blocks formatted.\n",
    "- Avoid extra line breaks or broken markdown syntax.\n",
    "\"\"\"\n",
    "\n",
    "def parse_single_pdf(pdf_path: str) -> list:\n",
    "    parser = LlamaParse(\n",
    "        api_key=LLAMA_API_KEY,\n",
    "        result_type=\"markdown\",\n",
    "        verbose=True,\n",
    "        parsing_instruction=parsing_instruction,\n",
    "    )\n",
    "    return parser.load_data(pdf_path)\n",
    "\n",
    "def save_markdown(docs, folder=\"markdown\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for i, doc in enumerate(docs):\n",
    "        with open(os.path.join(folder, f\"doc_{i+1}.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc.text)\n",
    "    print(f\" Saved {len(docs)} markdown files to '{folder}'\")\n",
    "\n",
    "pdf_path = \"sample.pdf\"  #  Provide full path to single PDF\n",
    "docs = parse_single_pdf(pdf_path)\n",
    "save_markdown(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644c8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document as LCDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4419ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 12 markdown documents\n"
     ]
    }
   ],
   "source": [
    "# Load markdown as LangChain documents\n",
    "def load_markdown_docs(folder=\"markdown\"):\n",
    "    loaded_docs = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".md\"):\n",
    "            with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            loaded_docs.append(LCDocument(page_content=text, metadata={\"source\": filename}))\n",
    "    return loaded_docs\n",
    "\n",
    "markdown_docs = load_markdown_docs()\n",
    "print(f\" Loaded {len(markdown_docs)} markdown documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bf1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2601f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "\n",
    "chunked_docs = []\n",
    "for doc in markdown_docs:\n",
    "    for i, chunk in enumerate(child_splitter.split_text(doc.page_content)):\n",
    "        chunked_docs.append(LCDocument(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": doc.metadata[\"source\"], \"chunk\": i}\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b4cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML(ExtraClass Project)\\RAG_PROJECT\\PDF-AI-Assistant-Chat-Summarize-Quiz-from-PDF\\.pdfassistantenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\arunp\\AppData\\Local\\Temp\\ipykernel_27304\\3286802395.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 71 chunks in Chroma vector store.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"md_chunks\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(chunked_docs)\n",
    "print(f\"Stored {len(chunked_docs)} chunks in Chroma vector store.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f11369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce60fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc424741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ParentDocumentRetriever ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "retriever.add_documents(markdown_docs)\n",
    "print(\" ParentDocumentRetriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67bcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f5ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chat_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Use the context below to answer the question.\n",
    "If the answer is not found, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert summarizer. Read the context and summarize key points, headings, and lists.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "quiz_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a quiz generator. Create {num_questions} MCQs from the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac74fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bd343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206a047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a8665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116ae9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatters\n",
    "context_retriever = RunnableLambda(lambda x: {\"context\": retriever.get_relevant_documents(x.get(\"question\", \"\")), **x})\n",
    "\n",
    "chat_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", [])),\n",
    "    \"question\": x.get(\"question\", \"\")\n",
    "})\n",
    "\n",
    "summarize_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", []))\n",
    "})\n",
    "\n",
    "quiz_formatter = RunnableLambda(lambda x: {\n",
    "    \"context\": \"\\n\\n\".join(doc.page_content for doc in x.get(\"context\", [])),\n",
    "    \"num_questions\": x.get(\"num_questions\", 5)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f08c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "chat_chain = RunnableSequence(context_retriever, chat_formatter, chat_prompt, llm, parser)\n",
    "summarize_chain = RunnableSequence(context_retriever, summarize_formatter, summarize_prompt, llm, parser)\n",
    "quiz_chain = RunnableSequence(context_retriever, quiz_formatter, quiz_prompt, llm, parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac9fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chat(x): return x.get(\"mode\") == \"chat\"\n",
    "def is_summary(x): return x.get(\"mode\") == \"summarize\"\n",
    "def is_quiz(x): return x.get(\"mode\") == \"quiz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a513c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RunnableBranch(\n",
    "    (is_chat, chat_chain),\n",
    "    (is_summary, summarize_chain),\n",
    "    (is_quiz, quiz_chain),\n",
    "    RunnableLambda(lambda _: \" Invalid mode. Choose 'chat', 'summarize', or 'quiz'.\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75551fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunp\\AppData\\Local\\Temp\\ipykernel_27304\\3374979016.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_retriever = RunnableLambda(lambda x: {\"context\": retriever.get_relevant_documents(x.get(\"question\", \"\")), **x})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Chat: A microcontroller is a highly integrated device which includes, on one chip, all or most of the parts needed to perform an application control function. It typically has bit manipulation instructions, easy and direct access to I/O, and quick and efficient interrupt processing.\n",
      "\n",
      "üßæ Summary: **Summary of Key Points and Headings**\n",
      "\n",
      "**Microcontrollers Made Easy**\n",
      "\n",
      "The article discusses the components and organization of microcontrollers, including:\n",
      "\n",
      "1. **Memory Types**:\n",
      "\t* Flash: electrically erasable and programmable memory\n",
      "\t* RAM (Random Access Memory): stores data temporarily during program execution\n",
      "\t* EEPROM (Electrically Erasable Programmable Read Only Memory): stores data that must be saved through a power down cycle\n",
      "2. **CPU (Central Processing Unit)**: the brain of the system that processes data and executes instructions\n",
      "3. **Communication**:\n",
      "\t* CAN (Controller Area Network) and J1850: multiplexed wiring schemes for automotive and industrial control applications\n",
      "\t* CAN specification: widely used in industrial control and has potential for growth with lower-cost microcontrollers\n",
      "\n",
      "**Key Figures and Tables**\n",
      "\n",
      "* Figure 2: Typical Microcontroller Block Organization\n",
      "* Figure 13: CAN Principle\n",
      "* Table: Inter-System Communication Speeds (Fast Speed, Slow Speed)\n",
      "\n",
      "**Key Concepts**\n",
      "\n",
      "* Endurance of Flash memory: 100 or 300,000 program/erase cycles\n",
      "* CPU execution process: fetch, decode, and execute instructions\n",
      "* Microcontroller organization: CPU, memory, and communication components\n",
      "\n",
      "üß† Quiz:\n",
      " Here are 3 MCQs based on the context:\n",
      "\n",
      "**Question 1**\n",
      "What is the primary difference between polling and interrupts in microcontrollers?\n",
      "\n",
      "A) Polling is faster than interrupts\n",
      "B) Polling is used for time-critical tasks, while interrupts are used for non-time-critical tasks\n",
      "C) Polling is a hardware technique, while interrupts are a software technique\n",
      "D) Polling is a software technique where peripherals tell the controller when they have data ready\n",
      "\n",
      "**Answer: D) Polling is a software technique where peripherals tell the controller when they have data ready**\n",
      "\n",
      "**Question 2**\n",
      "What happens when an event occurs in an interrupt-driven system?\n",
      "\n",
      "A) The controller continues to poll the peripherals\n",
      "B) The controller saves its state and processes the event\n",
      "C) The controller restores its state and continues with its normal function\n",
      "D) The controller ignores the event and continues with its normal function\n",
      "\n",
      "**Answer: B) The controller saves its state and processes the event**\n",
      "\n",
      "**Question 3**\n",
      "Why is polling considered less efficient than interrupts in microcontrollers?\n",
      "\n",
      "A) Because polling is faster than interrupts\n",
      "B) Because polling is used for time-critical tasks\n",
      "C) Because polling continually asks peripherals if they have data available, even when they do not\n",
      "D) Because polling is a hardware technique\n",
      "\n",
      "**Answer: C) Because polling continually asks peripherals if they have data available, even when they do not**\n"
     ]
    }
   ],
   "source": [
    "chat_response = rag_chain.invoke({\n",
    "    \"mode\": \"chat\",\n",
    "    \"question\": \"What is a microcontroller?\"\n",
    "})\n",
    "print(\"üó£Ô∏è Chat:\", chat_response)\n",
    "\n",
    "summary_response = rag_chain.invoke({\n",
    "    \"mode\": \"summarize\",\n",
    "    \"question\": \"Summarize this\"  # for retrieval\n",
    "})\n",
    "print(\"\\nüßæ Summary:\", summary_response)\n",
    "\n",
    "quiz_response = rag_chain.invoke({\n",
    "    \"mode\": \"quiz\",\n",
    "    \"question\": \"Generate quiz\",\n",
    "    \"num_questions\": 3\n",
    "})\n",
    "print(\"\\nüß† Quiz:\\n\", quiz_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146863d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pdfassistantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
